version: "3.5"
services:
  main:
    build:
      context: ./
      dockerfile: Dockerfile      
    ports:
      - "8004:8000"
    depends_on:
      - inference-service
      # - webapp
  inference-service:
    build:
      context: ./inference_service
    environment:
      - AWS_PROFILE=default
    volumes:
      - ~/.aws/:/root/.aws:ro        
    ports:
      - "8005:8000"
  # webapp:
  #   build:
  #     context: ./webapp
  #   ports:
  #     - "8080:8000"
  triton:
    image: nvcr.io/nvidia/tritonserver:22.02-py3
    runtime: nvidia
    env_file:
      - .aws.env
    ports:
      - "8001:8001" # part 8001 http, 8002 GRPC, 8003 Metric Service
      - "8002:8002"
      - "8003:8003"
    command:
      [
        "tritonserver",
        "--model-repository=s3://aerial-detection-mlops4/model/Visdrone/Yolov7/triton-deploy/models/",
        "--strict-model-config=false"
      ]
    shm_size: 1g
    ulimits:
      memlock: -1
      stack: 67108864