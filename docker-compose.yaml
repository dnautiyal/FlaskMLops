version: "3.5"
services:
  main:
    build:
      context: ./
      dockerfile: Dockerfile      
    ports:
      - "8004:8000"
    depends_on:
      - inference-service
      # - webapp
  inference-service:
    build:
      context: ./inference_service
    environment:
      - AWS_PROFILE=default
    volumes:
      - ~/.aws/:/root/.aws:ro        
    ports:
      - "8005:8000"
  # webapp:
  #   build:
  #     context: ./webapp
  #   ports:
  #     - "8080:8000"
  triton:
    image: nvcr.io/nvidia/tritonserver:22.02-py3
    env_file:
      - .aws.env
    ports:
      - "8001:8001" # part 8000 http, 8001 GRPC, 8002 Metric Service
    command:
      [
        "tritonserver",
        "--model-repository=s3://aerial-detection-mlops4/model/Visdrone/Yolov7/triton-deploy/models/",
        # "--gpus=all",
        "--rm",
        "--ipc=host",
        "--shm-size=1g",
        "--ulimit memlock=-1",
        "--ulimit stack=67108864",
        "-p8000:8000",
        "-p8001:8001",
        "-p8002:8002",
        "--strict-model-config=false",
        "--log-verbose 1"
      ]